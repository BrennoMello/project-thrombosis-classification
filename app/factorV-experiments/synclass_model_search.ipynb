{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SynClass classifier\n",
    "\n",
    "Date: July, 2022\n",
    "\n",
    "Developers:\n",
    "    Brenno Alencar,\n",
    "    Marcos Vinicius Ferreira\n",
    "    Ricardo Rios,\n",
    "    Tatiane Nogueira,\n",
    "    Tiago Lopes\n",
    "\n",
    "\n",
    "GNU General Public License v3.0\n",
    "\n",
    "Permissions of this strong copyleft license are \n",
    "\tconditioned on making available complete \n",
    "\tsource code of licensed works and \n",
    "\tmodifications, which include larger works \n",
    "\tusing a licensed work, under the same license. \n",
    "\tCopyright and license notices must be \n",
    "\tpreserved. Contributors provide an express \n",
    "\tgrant of patent rights.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import r\n",
    "from rpy2.robjects import numpy2ri\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from ialovecoffe.models import *\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def call_wilcox_R (x, y):\n",
    "    \"\"\"\n",
    "    Call R function wilcox.test() to perform wilcox test.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        First array of data.\n",
    "    y : numpy.ndarray\n",
    "        Second array of data.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pvalue : float\n",
    "        P-value of the test.\n",
    "\n",
    "    \"\"\"\n",
    "    r.assign(\"x\", x.to_numpy())\n",
    "    r.assign(\"y\", y.to_numpy())\n",
    "    r('res<-wilcox.test(x~y)$statistic')\n",
    "    r_result = r(\"res\")\n",
    "    return (r_result[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_statistical_weights(inputSet, labels):\n",
    "    \"\"\"\n",
    "    Get statistical weights for each class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inputSet : numpy.ndarray\n",
    "        Input data set.\n",
    "    ignore_class : float\n",
    "        Class to ignore.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weights : numpy.ndarray\n",
    "        Statistical weights for each class.\n",
    "        \n",
    "    \"\"\"    \n",
    "    myWeights = np.repeat(np.nan, inputSet.shape[1])\n",
    "\n",
    "    numpy2ri.activate()    \n",
    "    for i in np.arange(inputSet.shape[1]):\n",
    "        myWeights[i] = call_wilcox_R(inputSet.iloc[:,i], labels)\n",
    "\n",
    "    numpy2ri.deactivate()\n",
    "\n",
    "    return (1/myWeights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def auc_eval(y_test, y_pred, positive = 1):\n",
    "    \"\"\"\n",
    "    Calculate AUC per class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test : numpy.ndarray\n",
    "        True labels.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted labels.\n",
    "    positive : int\n",
    "        Index of positive class.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    auc : float\n",
    "        Area under the ROC curve.\n",
    "\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred, pos_label=positive)\n",
    "    return metrics.auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def accuracy_per_class(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate accuracy per class.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test : numpy.ndarray\n",
    "        True labels.\n",
    "    y_pred : numpy.ndarray\n",
    "        Predicted labels.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    acc_pos: float\n",
    "        Accuracy for the positive class.\n",
    "    acc_neg: float\n",
    "        Accuracy for the negative class.\n",
    "\n",
    "    \"\"\"\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    acc_pos = tp / (tp + fp)\n",
    "    acc_neg = tn / (tn + fn)\n",
    "\n",
    "    return acc_pos, acc_neg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weighted_mean(x, w):\n",
    "    \"\"\"\n",
    "    Calculate weighted mean.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        Array of values.\n",
    "    w : numpy.ndarray\n",
    "        Array of weights.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    weighted_mean : float\n",
    "        Weighted mean.\n",
    "\n",
    "    \"\"\"\n",
    "    return np.dot(x, w)/np.sum(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_calc_viz_pred(y_true, y_pred):\n",
    "    viz = RocCurveDisplay.from_predictions(\n",
    "                            y_true,\n",
    "                            y_pred\n",
    "                        )\n",
    "\n",
    "    return viz.fpr, viz.tpr, viz.roc_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SynClass(x_train, y_train, x_test, y_test, learner, scoring = 'auc', classThreshold = 0.5, probability = False):\n",
    "    \"\"\"\n",
    "    Perform SynClass.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train : numpy.ndarray\n",
    "        Training data.\n",
    "    y_train : numpy.ndarray\n",
    "        Training labels.\n",
    "    x_test : numpy.ndarray\n",
    "        Test data.\n",
    "    y_test : numpy.ndarray\n",
    "        Test labels.\n",
    "    learner : str\n",
    "        Learner to use.\n",
    "    scoring : str\n",
    "        Scoring method.\n",
    "    classThreshold : float\n",
    "        Class threshold.\n",
    "    seed : int\n",
    "        Random seed.\n",
    "    probability : bool\n",
    "        Whether to use probability or not.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    acc : float\n",
    "        Accuracy.        \n",
    "    auc : float\n",
    "        Area under the ROC curve.\n",
    "    f1 : float\n",
    "        F1 score.\n",
    "    \"\"\"\n",
    "    result_by_att = np.zeros((x_test.shape[0], x_train.shape[1]))\n",
    "\n",
    "    #fitting a model per attribute\n",
    "    for att in np.arange(x_train.shape[1]):\n",
    "        log.debug('-' * 30)\n",
    "        log.debug(f'Train with {att}')\n",
    "        log.debug('-' * 30)\n",
    "        result_by_att[:, att], _, _ = learner(x_train.iloc[:, att], y_train, x_test.iloc[:, att], y_test, prob=probability, metric=scoring)\n",
    "            \n",
    "\n",
    "    statistical_weights = get_statistical_weights(x_train, y_train)\n",
    "    prediction = np.apply_along_axis(weighted_mean, 1, result_by_att, w = statistical_weights)\n",
    "\n",
    "    y_pred = np.repeat(0, x_test.shape[0])\n",
    "    y_pred[np.where(prediction > classThreshold)] = 1\n",
    "\n",
    "    acc_pos, acc_neg = accuracy_per_class(y_test, y_pred)\n",
    "\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    auc = auc_eval(y_test, y_pred)\n",
    "    acc = [acc_pos, acc_neg]\n",
    "\n",
    "    return acc, f1, auc, prediction, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SynClass(x_train, y_train, x_test, y_test, learner, scoring = 'auc', classThreshold = 0.5, probability = False, rangeThreshold = [0.1, 0.81, 0.01], results = {}):\n",
    "    \"\"\"\n",
    "    Perform SynClass.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x_train : numpy.ndarray\n",
    "        Training data.\n",
    "    y_train : numpy.ndarray\n",
    "        Training labels.\n",
    "    x_test : numpy.ndarray\n",
    "        Test data.\n",
    "    y_test : numpy.ndarray\n",
    "        Test labels.\n",
    "    learner : str\n",
    "        Learner to use.\n",
    "    scoring : str\n",
    "        Scoring method.\n",
    "    classThreshold : float\n",
    "        Class threshold.\n",
    "    seed : int\n",
    "        Random seed.\n",
    "    probability : bool\n",
    "        Whether to use probability or not.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    acc : float\n",
    "        Accuracy.        \n",
    "    auc : float\n",
    "        Area under the ROC curve.\n",
    "    f1 : float\n",
    "        F1 score.\n",
    "    \"\"\"\n",
    "    result_by_att = np.zeros((x_test.shape[0], x_train.shape[1]))\n",
    "\n",
    "    #classifiers = dict()\n",
    "\n",
    "    #fitting a model per attribute'ACC':[],\n",
    "    for att in np.arange(x_train.shape[1]):\n",
    "        log.debug('-' * 30)\n",
    "        log.debug(f'Train with {att}')\n",
    "        log.debug('-' * 30)\n",
    "\n",
    "        result_by_att[:, att], _, _ = learner(x_train.iloc[:, att], y_train, x_test.iloc[:, att], y_test, prob=probability, metric=scoring)\n",
    "        #classifiers[att] = model_cv\n",
    "\n",
    "    for thresh in np.arange(start=rangeThreshold[0], stop=rangeThreshold[1], step=rangeThreshold[2]):\n",
    "        log.debug('-' * 30)\n",
    "        log.debug(f'Threshold with {thresh}')\n",
    "        \n",
    "        statistical_weights = get_statistical_weights(x_train, y_train)\n",
    "        prediction = np.apply_along_axis(weighted_mean, 1, result_by_att, w = statistical_weights)\n",
    "\n",
    "        y_pred = np.repeat(0, x_test.shape[0])\n",
    "        y_pred[np.where(prediction > thresh)] = 1\n",
    "        \n",
    "        acc_pos, acc_neg = accuracy_per_class(y_test, y_pred)\n",
    "\n",
    "        f1 = f1_score(y_test, y_pred, average='macro')\n",
    "        auc = auc_eval(y_test, y_pred)\n",
    "        acc = [acc_pos, acc_neg]\n",
    "        viz_fpr, viz_tpr, viz_auc = roc_calc_viz_pred(y_test, prediction)\n",
    "\n",
    "        log.debug(f'SynClass F1 ......: {f1}')\n",
    "        log.debug('-' * 30)\n",
    "\n",
    "        results['model_name'].append('SynClass')\n",
    "        results['acc-class-1'].append(acc_pos)\n",
    "        results['acc-class-2'].append(acc_neg)\n",
    "        results['F1'].append(f1)\n",
    "        results['ROC'].append(auc)\n",
    "        results['FPR'].append(viz_fpr)\n",
    "        results['TPR'].append(viz_tpr)\n",
    "        results['AUC'].append(viz_auc)\n",
    "        results['THRE'].append(thresh)\n",
    "\n",
    "    return results\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.14 64-bit ('3.8.14')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.14"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "82c9c2c57c601d68e690665a581a4f6b257e61a880e65e7cc718848198e93d9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
